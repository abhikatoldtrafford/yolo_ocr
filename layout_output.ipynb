{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91ea9b6d-644e-4859-b8f2-e02f5afd6aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import torch\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "def get_iou(boxA, boxB):\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    return iou\n",
    "\n",
    "def get_iou_polygon(polygonA, polygonB):\n",
    "    # This function would calculate the IoU between two polygons.\n",
    "    polyA = Polygon([(polygonA[i], polygonA[i + 1]) for i in range(0, len(polygonA), 2)])\n",
    "    polyB = Polygon([(polygonB[i], polygonB[i + 1]) for i in range(0, len(polygonB), 2)])\n",
    "    iou = polyA.intersection(polyB).area / polyA.union(polyB).area\n",
    "    return iou\n",
    "\n",
    "def nms_classwise(df):\n",
    "    def nms_single_class(df_class):\n",
    "        elements = df_class.to_dict('records')\n",
    "        elements.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "        selected_elements = []\n",
    "\n",
    "        for current in elements:\n",
    "            # Correctly using list comprehension for creating a list of vertices for the current polygon\n",
    "            current_polygon = [val for i in range(4) for val in (current[f'x{i+1}'], current[f'y{i+1}'])]\n",
    "\n",
    "            overlap = False\n",
    "            for selected in selected_elements:\n",
    "                # Similarly constructing the polygon vertices list for the selected element\n",
    "                selected_polygon = [val for i in range(4) for val in (selected[f'x{i+1}'], selected[f'y{i+1}'])]\n",
    "\n",
    "                if get_iou_polygon(current_polygon, selected_polygon) > 0.3:  # Using the corrected polygon IoU function\n",
    "                    overlap = True\n",
    "                    break\n",
    "\n",
    "            if not overlap:\n",
    "                selected_elements.append(current)\n",
    "                \n",
    "        return pd.DataFrame(selected_elements)\n",
    "\n",
    "    grouped = df.groupby('label')\n",
    "    nms_dfs = [nms_single_class(group) for _, group in grouped]\n",
    "    return pd.concat(nms_dfs, ignore_index=True)\n",
    "\n",
    "def get_symmetrical_angle_ranges_with_median(df):\n",
    "    pivot = 90\n",
    "    # Adjust all angles to be within 0-90 based on their symmetry around the pivot\n",
    "    df['adjusted_angle'] = df['angle'].apply(lambda x: x if x <= pivot else 180 - x)\n",
    "    bin_counts = np.histogram(df['adjusted_angle'], bins=[0, 15, 30, 45, 60, 75, 90])[0]\n",
    "    top_two_bins = bin_counts.argsort()[-2:][::-1]  # Get indexes of the two highest bins\n",
    "    total_count_top_two_bins = sum(bin_counts[top_two_bins])\n",
    "    if total_count_top_two_bins / len(df) * 100 > 80:\n",
    "        median_rotation = np.median(df['adjusted_angle'])\n",
    "        return True, median_rotation\n",
    "    else:\n",
    "        return False, None\n",
    "\n",
    "\n",
    "def get_yolo_boxes(image_path, cropped_img = None, dump_images=False, conf_thresh = .25, estimate_rotation = True):\n",
    "    try:\n",
    "        im = np.array(cropped_img)\n",
    "        im = im[:, :, ::-1].copy()\n",
    "    except:\n",
    "        im = cv2.imread(image_path)\n",
    "    predictions = det_model(im, imgsz=800, conf=0.05, max_det=10000, verbose=False)\n",
    "    xywhr =  predictions[0].obb.xywhr.tolist()\n",
    "    xyxyxyxy = predictions[0].obb.xyxyxyxy.tolist()\n",
    "    xyxy = predictions[0].obb.xyxy.tolist()\n",
    "    classes = predictions[0].obb.cls.tolist()  \n",
    "    confidence = predictions[0].obb.conf.tolist()\n",
    "    names = [\n",
    "        \"box\", \"table\", \"column\", \"header\",\n",
    "        \"signature\", \"figure\", \"paragraph\", \"logo\", \"kv\", \"stamp\"\n",
    "    ] # Get class names\n",
    "    name_to_id = {name: i for i, name in enumerate(names)}\n",
    "\n",
    "    # Color codes for each label\n",
    "    color_codes = {\n",
    "        \"box\": (255, 0, 0),  # Blue\n",
    "        \"header\": (255, 0, 0),  # Blue\n",
    "        \"table\": (0, 255, 0),  # Green\n",
    "        \"column\": (0, 255, 0),  # Green\n",
    "        \"logo\": (0, 0, 255),  # Red\n",
    "        \"signature\": (0, 255, 255),  # Yellow\n",
    "        \"stamp\": (0, 0, 0),  # Black\n",
    "        # Default color for other labels (optional)\n",
    "        \"default\": (255, 255, 255)  # White\n",
    "    }\n",
    "    if dump_images:\n",
    "        # Check if results directory exists, if not, create it\n",
    "        if not os.path.exists('results'):\n",
    "            os.makedirs('results')\n",
    "\n",
    "        # Make a copy of the image to draw on\n",
    "        im_copy = im.copy()\n",
    "    df_data = []\n",
    "    yolo_data = []\n",
    "    for i, a in enumerate(xywhr):\n",
    "        centroid_x, centroid_y, width, height, radians = a\n",
    "        x1, y1 = int(xyxyxyxy[i][0][0]), int(xyxyxyxy[i][0][1])\n",
    "        x2, y2 = int(xyxyxyxy[i][1][0]), int(xyxyxyxy[i][1][1])\n",
    "        x3, y3 = int(xyxyxyxy[i][2][0]), int(xyxyxyxy[i][2][1])\n",
    "        x4, y4 = int(xyxyxyxy[i][3][0]), int(xyxyxyxy[i][3][1])\n",
    "        \n",
    "        label_id = int(classes[i])\n",
    "        label_name = names[label_id]\n",
    "        conf = confidence[i]\n",
    "\n",
    "        if conf > conf_thresh:\n",
    "            df_data.append({\n",
    "                \"x1\": int(x1), \"y1\": int(y1),\n",
    "                \"x2\": int(x2), \"y2\": int(y2),\n",
    "                \"x3\": int(x3), \"y3\": int(y3),\n",
    "                \"x4\": int(x4), \"y4\": int(y4),\n",
    "                \"centroid_x\": centroid_x, \"centroid_y\": centroid_y,\n",
    "                \"width\": width, \"height\": height, \"angle\": radians * 180 / 3.14159,\n",
    "                \"label\": label_name, \"confidence\": conf\n",
    "            })\n",
    "     \n",
    "    df = pd.DataFrame(df_data, columns=['x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'x4', 'y4', 'centroid_x', 'centroid_y', 'width', 'height', 'angle', 'label', 'confidence'])\n",
    "    \n",
    "    if len(df) > 0:\n",
    "        filtered_df = nms_classwise(df)\n",
    "        if dump_images and len(df) > 0:\n",
    "        # Iterate over each row in the filtered DataFrame to draw the polygons\n",
    "            for index, row in filtered_df.iterrows():\n",
    "            # Extract points for the current polygon\n",
    "                pts = np.array([(row['x1'], row['y1']), (row['x2'], row['y2']),\n",
    "                            (row['x3'], row['y3']), (row['x4'], row['y4'])], np.int32)\n",
    "            # Reshape points in the form required by polylines\n",
    "                pts = pts.reshape((-1, 1, 2))\n",
    "            \n",
    "            # Use specified color or default if not specified\n",
    "                color = color_codes.get(row['label'], color_codes[\"default\"])\n",
    "            \n",
    "            # Draw the polygon on the image\n",
    "                cv2.polylines(im_copy, [pts], isClosed=True, color=color, thickness=2)\n",
    "\n",
    "        # Save the modified image\n",
    "            output_image_path = os.path.join('results', os.path.basename(image_path))\n",
    "            cv2.imwrite(output_image_path, im_copy)\n",
    "    else:\n",
    "        return df, 0\n",
    "    final_rotation = 0\n",
    "    if estimate_rotation:\n",
    "        has_rotation, final_rotation = get_symmetrical_angle_ranges_with_median(df)\n",
    "    return filtered_df, final_rotation\n",
    "\n",
    "\n",
    "det_model = YOLO(\"rotation_v2.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "508d0fb5-d5c0-429f-ace3-cf3fcb269c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▌                                                                                                                                     | 2/477 [00:03<13:02,  1.65s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_path \u001b[38;5;129;01min\u001b[39;00m tqdm(glob\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages/*.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# for img_path in ['images/um_10-01-05-006.jpg', 'images/10-15-06-001.jpg']:\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     df, page_rotation \u001b[38;5;241m=\u001b[39m \u001b[43mget_yolo_boxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf_thresh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimate_rotation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdump_images\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 139\u001b[0m, in \u001b[0;36mget_yolo_boxes\u001b[1;34m(image_path, cropped_img, dump_images, conf_thresh, estimate_rotation)\u001b[0m\n\u001b[0;32m    136\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(df_data, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcentroid_x\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcentroid_y\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mangle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m     filtered_df \u001b[38;5;241m=\u001b[39m \u001b[43mnms_classwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dump_images \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;66;03m# Iterate over each row in the filtered DataFrame to draw the polygons\u001b[39;00m\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m filtered_df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;66;03m# Extract points for the current polygon\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 58\u001b[0m, in \u001b[0;36mnms_classwise\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(selected_elements)\n\u001b[0;32m     57\u001b[0m grouped \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m nms_dfs \u001b[38;5;241m=\u001b[39m [nms_single_class(group) \u001b[38;5;28;01mfor\u001b[39;00m _, group \u001b[38;5;129;01min\u001b[39;00m grouped]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat(nms_dfs, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[2], line 58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(selected_elements)\n\u001b[0;32m     57\u001b[0m grouped \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m nms_dfs \u001b[38;5;241m=\u001b[39m [\u001b[43mnms_single_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _, group \u001b[38;5;129;01min\u001b[39;00m grouped]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat(nms_dfs, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[2], line 48\u001b[0m, in \u001b[0;36mnms_classwise.<locals>.nms_single_class\u001b[1;34m(df_class)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m selected \u001b[38;5;129;01min\u001b[39;00m selected_elements:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# Similarly constructing the polygon vertices list for the selected element\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     selected_polygon \u001b[38;5;241m=\u001b[39m [val \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m (selected[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m], selected[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m])]\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mget_iou_polygon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_polygon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_polygon\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.3\u001b[39m:  \u001b[38;5;66;03m# Using the corrected polygon IoU function\u001b[39;00m\n\u001b[0;32m     49\u001b[0m         overlap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 30\u001b[0m, in \u001b[0;36mget_iou_polygon\u001b[1;34m(polygonA, polygonB)\u001b[0m\n\u001b[0;32m     28\u001b[0m polyA \u001b[38;5;241m=\u001b[39m Polygon([(polygonA[i], polygonA[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(polygonA), \u001b[38;5;241m2\u001b[39m)])\n\u001b[0;32m     29\u001b[0m polyB \u001b[38;5;241m=\u001b[39m Polygon([(polygonB[i], polygonB[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(polygonB), \u001b[38;5;241m2\u001b[39m)])\n\u001b[1;32m---> 30\u001b[0m iou \u001b[38;5;241m=\u001b[39m \u001b[43mpolyA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintersection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolyB\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39marea \u001b[38;5;241m/\u001b[39m polyA\u001b[38;5;241m.\u001b[39munion(polyB)\u001b[38;5;241m.\u001b[39marea\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m iou\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\paddle_env\\lib\\site-packages\\shapely\\geometry\\base.py:599\u001b[0m, in \u001b[0;36mBaseGeometry.intersection\u001b[1;34m(self, other, grid_size)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mintersection\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, grid_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    594\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;124;03m    Returns the intersection of the geometries.\u001b[39;00m\n\u001b[0;32m    596\u001b[0m \n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03m    Refer to `shapely.intersection` for full documentation.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mshapely\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintersection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\paddle_env\\lib\\site-packages\\shapely\\decorators.py:77\u001b[0m, in \u001b[0;36mmultithreading_enabled.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m array_args:\n\u001b[0;32m     76\u001b[0m         arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arr, old_flag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(array_args, old_flags):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\paddle_env\\lib\\site-packages\\shapely\\set_operations.py:131\u001b[0m, in \u001b[0;36mintersection\u001b[1;34m(a, b, grid_size, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrid_size parameter only accepts scalar values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mintersection_prec(a, b, grid_size, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintersection\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for img_path in tqdm(glob.glob('images/*.jpg')):\n",
    "# for img_path in ['images/um_10-01-05-006.jpg', 'images/10-15-06-001.jpg']:\n",
    "    df, page_rotation = get_yolo_boxes(img_path, conf_thresh = .1, estimate_rotation = True, dump_images = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d75a2e32-8614-4974-9797-60c8f7a11894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "def extract_easyocr(image_path, mode=['LINE']):\n",
    "    reader = easyocr.Reader(['en'], gpu=False)  # Initialize EasyOCR with English on CPU\n",
    "    results = reader.readtext(image_path, detail=1)  # detail=1 for bounding box and confidence\n",
    "\n",
    "    img = Image.open(image_path)\n",
    "    width, height = img.size\n",
    "    data = []\n",
    "    \n",
    "    # Process results\n",
    "    for result in results:\n",
    "        # Each result in EasyOCR output with detail=1 is (box, text, confidence)\n",
    "        if result:\n",
    "            box = result[0]\n",
    "            text = result[1]\n",
    "            confidence = result[2]\n",
    "\n",
    "            # Extract all x and y coordinates from the box\n",
    "            x_coords = [int(point[0]) for point in box]\n",
    "            y_coords = [int(point[1]) for point in box]\n",
    "\n",
    "            # Find the minimum and maximum x and y coordinates\n",
    "            x1 = min(x_coords)\n",
    "            y1 = min(y_coords)\n",
    "            x2 = max(x_coords)\n",
    "            y2 = max(y_coords)\n",
    "\n",
    "            # Store data in a dictionary and append to list\n",
    "            data.append({\n",
    "                'x1': x1,\n",
    "                'y1': y1,\n",
    "                'x2': x2,\n",
    "                'y2': y2,\n",
    "                'text': text,\n",
    "                'confidence': confidence\n",
    "            })\n",
    "\n",
    "    # Create a DataFrame from the list of dictionaries\n",
    "    df = pd.DataFrame(data, columns=['x1', 'y1', 'x2', 'y2', 'text', 'confidence'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a12c05c9-d404-43b6-82a1-2962910fbf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                              | 0/412 [00:00<?, ?it/s]Using CPU. Note: This module is much faster with a GPU.\n",
      "Using CPU. Note: This module is much faster with a GPU.\n",
      "  0%|▎                                                                                                                                   | 1/412 [00:20<2:19:23, 20.35s/it]Using CPU. Note: This module is much faster with a GPU.\n",
      "Using CPU. Note: This module is much faster with a GPU.\n",
      "  0%|▋                                                                                                                                   | 2/412 [00:40<2:19:49, 20.46s/it]Using CPU. Note: This module is much faster with a GPU.\n",
      "  0%|▋                                                                                                                                   | 2/412 [00:48<2:45:47, 24.26s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 320\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_path \u001b[38;5;129;01min\u001b[39;00m tqdm(glob\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrectified/*.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m--> 320\u001b[0m     df_textract \u001b[38;5;241m=\u001b[39m \u001b[43mextract_easyocr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m     df_yolo \u001b[38;5;241m=\u001b[39m get_yolo_boxes(image_path)\n\u001b[0;32m    322\u001b[0m     match_and_draw(image_path, df_textract, df_yolo)\n",
      "Cell \u001b[1;32mIn[19], line 6\u001b[0m, in \u001b[0;36mextract_easyocr\u001b[1;34m(image_path, mode)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_easyocr\u001b[39m(image_path, mode\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLINE\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m      5\u001b[0m     reader \u001b[38;5;241m=\u001b[39m easyocr\u001b[38;5;241m.\u001b[39mReader([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m], gpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# Initialize EasyOCR with English on CPU\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetail\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# detail=1 for bounding box and confidence\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\n\u001b[0;32m      9\u001b[0m     width, height \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39msize\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\paddle_env\\lib\\site-packages\\easyocr\\easyocr.py:456\u001b[0m, in \u001b[0;36mReader.readtext\u001b[1;34m(self, image, decoder, beamWidth, batch_size, workers, allowlist, blocklist, detail, rotation_info, paragraph, min_size, contrast_ths, adjust_contrast, filter_ths, text_threshold, low_text, link_threshold, canvas_size, mag_ratio, slope_ths, ycenter_ths, height_ths, width_ths, y_ths, x_ths, add_margin, threshold, bbox_min_score, bbox_min_size, max_candidates, output_format)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;124;03mimage: file path or numpy-array or a byte stream object\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    454\u001b[0m img, img_cv_grey \u001b[38;5;241m=\u001b[39m reformat_input(image)\n\u001b[1;32m--> 456\u001b[0m horizontal_list, free_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mmin_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmin_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtext_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mlow_text\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlow_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlink_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlink_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mcanvas_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcanvas_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmag_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmag_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mslope_ths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mslope_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mycenter_ths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mycenter_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mheight_ths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mheight_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth_ths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwidth_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43madd_margin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43madd_margin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreformat\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_min_score\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbbox_min_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mbbox_min_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbbox_min_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_candidates\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_candidates\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;66;03m# get the 1st result from hor & free list as self.detect returns a list of depth 3\u001b[39;00m\n\u001b[0;32m    467\u001b[0m horizontal_list, free_list \u001b[38;5;241m=\u001b[39m horizontal_list[\u001b[38;5;241m0\u001b[39m], free_list[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\paddle_env\\lib\\site-packages\\easyocr\\easyocr.py:321\u001b[0m, in \u001b[0;36mReader.detect\u001b[1;34m(self, img, min_size, text_threshold, low_text, link_threshold, canvas_size, mag_ratio, slope_ths, ycenter_ths, height_ths, width_ths, add_margin, reformat, optimal_num_chars, threshold, bbox_min_score, bbox_min_size, max_candidates)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reformat:\n\u001b[0;32m    319\u001b[0m     img, img_cv_grey \u001b[38;5;241m=\u001b[39m reformat_input(img)\n\u001b[1;32m--> 321\u001b[0m text_box_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_textbox\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcanvas_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcanvas_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mmag_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmag_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtext_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtext_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mlink_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlink_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mlow_text\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlow_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mpoly\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m                            \u001b[49m\u001b[43moptimal_num_chars\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimal_num_chars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mbbox_min_score\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbbox_min_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mbbox_min_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbbox_min_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mmax_candidates\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_candidates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m                            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    337\u001b[0m horizontal_list_agg, free_list_agg \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text_box \u001b[38;5;129;01min\u001b[39;00m text_box_list:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\paddle_env\\lib\\site-packages\\easyocr\\detection.py:95\u001b[0m, in \u001b[0;36mget_textbox\u001b[1;34m(detector, image, canvas_size, mag_ratio, text_threshold, link_threshold, low_text, poly, device, optimal_num_chars, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     94\u001b[0m estimate_num_chars \u001b[38;5;241m=\u001b[39m optimal_num_chars \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m bboxes_list, polys_list \u001b[38;5;241m=\u001b[39m \u001b[43mtest_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcanvas_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmag_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mlink_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimate_num_chars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimate_num_chars:\n\u001b[0;32m    100\u001b[0m     polys_list \u001b[38;5;241m=\u001b[39m [[p \u001b[38;5;28;01mfor\u001b[39;00m p, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(polys, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mabs\u001b[39m(optimal_num_chars \u001b[38;5;241m-\u001b[39m x[\u001b[38;5;241m1\u001b[39m]))]\n\u001b[0;32m    101\u001b[0m                   \u001b[38;5;28;01mfor\u001b[39;00m polys \u001b[38;5;129;01min\u001b[39;00m polys_list]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\paddle_env\\lib\\site-packages\\easyocr\\detection.py:46\u001b[0m, in \u001b[0;36mtest_net\u001b[1;34m(canvas_size, mag_ratio, net, image, text_threshold, link_threshold, low_text, poly, device, estimate_num_chars)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 46\u001b[0m     y, feature \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m boxes_list, polys_list \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m out \u001b[38;5;129;01min\u001b[39;00m y:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# make score and link map\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\paddle_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\paddle_env\\lib\\site-packages\\easyocr\\craft.py:60\u001b[0m, in \u001b[0;36mCRAFT.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Base network \"\"\"\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     sources \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasenet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" U network \"\"\"\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([sources[\u001b[38;5;241m0\u001b[39m], sources[\u001b[38;5;241m1\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\paddle_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\paddle_env\\lib\\site-packages\\easyocr\\model\\modules.py:74\u001b[0m, in \u001b[0;36mvgg16_bn.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     72\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslice3(h)\n\u001b[0;32m     73\u001b[0m h_relu4_3 \u001b[38;5;241m=\u001b[39m h\n\u001b[1;32m---> 74\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslice4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m h_relu5_3 \u001b[38;5;241m=\u001b[39m h\n\u001b[0;32m     76\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslice5(h)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\paddle_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\paddle_env\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\paddle_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\paddle_env\\lib\\site-packages\\torch\\nn\\modules\\conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\paddle_env\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    441\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    442\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw, ImageOps, ImageEnhance, ImageFont\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError, PartialCredentialsError\n",
    "import pandas as pd\n",
    "import fastdeploy as fd\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import easyocr\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "det_model = fd.vision.detection.YOLOv8('stamp_layout_rotation.onnx')\n",
    "det_model.preprocessor.size = [800, 800]\n",
    "\n",
    "def extract_easyocr(image_path, mode=['LINE']):\n",
    "    reader = easyocr.Reader(['en'], gpu=False)  # Initialize EasyOCR with English on CPU\n",
    "    results = reader.readtext(image_path, detail=1)  # detail=1 for bounding box and confidence\n",
    "\n",
    "    img = Image.open(image_path)\n",
    "    width, height = img.size\n",
    "    data = []\n",
    "    \n",
    "    # Process results\n",
    "    for result in results:\n",
    "        # Each result in EasyOCR output with detail=1 is (box, text, confidence)\n",
    "        if result:\n",
    "            box = result[0]\n",
    "            text = result[1]\n",
    "            confidence = result[2]\n",
    "\n",
    "            # Extract all x and y coordinates from the box\n",
    "            x_coords = [int(point[0]) for point in box]\n",
    "            y_coords = [int(point[1]) for point in box]\n",
    "\n",
    "            # Find the minimum and maximum x and y coordinates\n",
    "            x1 = min(x_coords)\n",
    "            y1 = min(y_coords)\n",
    "            x2 = max(x_coords)\n",
    "            y2 = max(y_coords)\n",
    "\n",
    "            # Store data in a dictionary and append to list\n",
    "            data.append({\n",
    "                'x1': x1,\n",
    "                'y1': y1,\n",
    "                'x2': x2,\n",
    "                'y2': y2,\n",
    "                'text': text,\n",
    "                'confidence': confidence\n",
    "            })\n",
    "\n",
    "    # Create a DataFrame from the list of dictionaries\n",
    "    df = pd.DataFrame(data, columns=['x1', 'y1', 'x2', 'y2', 'text', 'confidence'])\n",
    "    return df\n",
    "def get_yolo_boxes(image_path, dump_boxes=False):\n",
    "    im = cv2.imread(image_path)\n",
    "    h, w, _ = im.shape  # Get the dimensions of the image\n",
    "    result = det_model.predict(im)\n",
    "\n",
    "    names = [\n",
    "        \"box\", \"table\", \"column\", \"header\",\n",
    "        \"signature\", \"figure\", \"paragraph\", \"logo\", \"kv\", \"stamp\"\n",
    "    ]\n",
    "    \n",
    "    # Mapping from label names to IDs (0-indexed)\n",
    "    name_to_id = {name: i for i, name in enumerate(names)}\n",
    "\n",
    "    df_data = []\n",
    "    yolo_data = []\n",
    "\n",
    "    for i in range(len(result.boxes)):\n",
    "        xmin, ymin, xmax, ymax = result.boxes[i]\n",
    "        label_id = result.label_ids[i]\n",
    "        confidence = result.scores[i]\n",
    "        if confidence > .25:\n",
    "            df_data.append({\n",
    "                \"x1\": int(xmin),\n",
    "                \"y1\": int(ymin),\n",
    "                \"x2\": int(xmax),\n",
    "                \"y2\": int(ymax),\n",
    "                \"label\": names[label_id],  # Use label name\n",
    "                \"confidence\": confidence\n",
    "            })\n",
    "            if dump_boxes:\n",
    "                # Calculate normalized values\n",
    "                x_center = ((xmin + xmax) / 2) / w\n",
    "                y_center = ((ymin + ymax) / 2) / h\n",
    "                width = (xmax - xmin) / w\n",
    "                height = (ymax - ymin) / h\n",
    "                # Append as list to ensure correct data types\n",
    "                yolo_data.append([name_to_id[names[label_id]], x_center, y_center, width, height])\n",
    "    \n",
    "    # Convert list of data to DataFrame\n",
    "    df = pd.DataFrame(df_data, columns=['x1', 'y1', 'x2', 'y2', 'label', 'confidence'])\n",
    "\n",
    "    if dump_boxes:\n",
    "        # Convert YOLO data to DataFrame for consistent formatting\n",
    "        yolo_df = pd.DataFrame(yolo_data, columns=['class_id', 'x_center', 'y_center', 'width', 'height'])\n",
    "        # Ensure data types are correctly set: class_id as int, others as float\n",
    "        yolo_df = yolo_df.astype({'class_id': int, 'x_center': float, 'y_center': float, 'width': float, 'height': float})\n",
    "        txt_path = image_path.rsplit('.', 1)[0] + '.txt'  # Supports multiple image extensions\n",
    "        # Dump using pandas to_csv for consistent formatting, ensure no index and header, and specify float format\n",
    "        yolo_df.to_csv(txt_path, sep=' ', index=False, header=False)\n",
    "    \n",
    "    return df  # Return the path to the .txt file for confirmation\n",
    "\n",
    "\n",
    "def get_iou(boxA, boxB):\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    return iou\n",
    "\n",
    "\n",
    "def inside_iou(det_box, parent_box, return_iou=False):\n",
    "    xi1 = max(det_box[0], parent_box[0])\n",
    "    yi1 = max(det_box[1], parent_box[1])\n",
    "    xi2 = min(det_box[2], parent_box[2])\n",
    "    yi2 = min(det_box[3], parent_box[3])\n",
    "    inter_area = max(xi2 - xi1, 0) * max(yi2 - yi1, 0)\n",
    "    \n",
    "    det_area = (det_box[2] - det_box[0]) * (det_box[3] - det_box[1])\n",
    "    iou = inter_area / float(det_area) if det_area != 0 else 0\n",
    "    if return_iou:\n",
    "        return iou\n",
    "    else:\n",
    "        return iou > 0.5\n",
    "\n",
    "def filter_boundingbox_output(df_textract):\n",
    "    # Convert DataFrame rows to a list of dictionaries for easier manipulation\n",
    "    textract_data = df_textract.to_dict('records')\n",
    "    filtered_boxes = []\n",
    "    skip_indices = set()\n",
    "    for i, box_a in enumerate(textract_data):\n",
    "        if i in skip_indices:  # Skip this box if it's already marked as contained within another\n",
    "            continue\n",
    "        for j, box_b in enumerate(textract_data):\n",
    "            if i != j and j not in skip_indices:\n",
    "                # Check if box_b is completely inside box_a\n",
    "                if inside_iou([box_b['x1'], box_b['y1'], box_b['x2'], box_b['y2']], \n",
    "                              [box_a['x1'], box_a['y1'], box_a['x2'], box_a['y2']], return_iou=True) == 1:\n",
    "                    # Mark box_b to be skipped in future iterations\n",
    "                    skip_indices.add(j)\n",
    "        # After checking all boxes for containment, add the current box to the filtered list\n",
    "        filtered_boxes.append(box_a)\n",
    "    # Convert the filtered list of boxes back to a DataFrame\n",
    "    df_textract_filtered = pd.DataFrame(filtered_boxes)\n",
    "    return df_textract_filtered\n",
    "\n",
    "def calculate_composite_dimensions(boxes, horizontal_padding, vertical_buffer, font_size, text_width=100, gap = 5):\n",
    "    \"\"\"\n",
    "    Calculate the dimensions for the composite image, including additional space for text.\n",
    "    \"\"\"\n",
    "    # Load the font and calculate text size for a representative string\n",
    "    font = ImageFont.truetype(\"arial.ttf\", font_size)\n",
    "    text_height = max(textsize(f\"{box[0]}, {box[1]}, {box[2]}, {box[3]}\", font=font)[1] for box in boxes)\n",
    "    \n",
    "    max_padded_width = max(box[2] - box[0] for box in boxes) + 2 * horizontal_padding + text_width\n",
    "    total_height = sum(box[3] - box[1] + text_height + gap for box in boxes)\n",
    "    \n",
    "    return max_padded_width, total_height\n",
    "\n",
    "def textsize(text, font):\n",
    "    im = Image.new(mode=\"P\", size=(0, 0))\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    _, _, width, height = draw.textbbox((0, 0), text=text, font=font)\n",
    "    return width, height\n",
    "def get_median_crop_color(crops):\n",
    "    # Convert crops to grayscale and flatten the list of pixel values\n",
    "    pixels = np.concatenate([np.array(crop.convert('L')).flatten() for crop in crops])\n",
    "    # Calculate the median pixel value\n",
    "    median_pixel = int(np.median(pixels))\n",
    "    return (median_pixel, median_pixel, median_pixel)  # Return as RGB for compatibility\n",
    "def parse_extracted_text(extracted_text):\n",
    "    rows = []\n",
    "    last_valid_coords = None\n",
    "\n",
    "    for line in extracted_text:\n",
    "        if ':' in line:\n",
    "            coordinates_part, text_part = line.split(':', 1)\n",
    "        else:\n",
    "            # Handle case with no colon (e.g., text continuation)\n",
    "            coordinates_part, text_part = '', line\n",
    "\n",
    "        # Clean and split the coordinates part\n",
    "        coord_strs = coordinates_part.replace('.', ',').strip().split(',')\n",
    "        text_str = text_part.strip()\n",
    "\n",
    "        # Try parsing coordinates\n",
    "        try:\n",
    "            coords = [int(coord.strip()) for coord in coord_strs if coord.strip()]\n",
    "            if len(coords) == 4:\n",
    "                last_valid_coords = {'x1': coords[0], 'y1': coords[1], 'x2': coords[2], 'y2': coords[3], 'text': text_str}\n",
    "                rows.append(last_valid_coords)\n",
    "            else:\n",
    "                raise ValueError(\"Not enough valid coordinates\")\n",
    "        except ValueError:\n",
    "            # Handle invalid or missing coordinates\n",
    "            if last_valid_coords and text_str:\n",
    "                # If there's pending text without coordinates, append it to the last valid entry's text\n",
    "                last_valid_coords['text'] += ' ' + text_str\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "def crop_and_extract_text(image_path, boxes, gap=0, horizontal_padding=10, vertical_buffer=10, minimal_gap=5, target_text_height=30):\n",
    "    original_img = Image.open(image_path)\n",
    "    crops = [original_img.crop((box[0], box[1], box[2], box[3])) for box in boxes]\n",
    "    median_color = get_median_crop_color(crops)\n",
    "    font = ImageFont.truetype(\"arial.ttf\", target_text_height)  # Use target_text_height as an initial guess\n",
    "    fontwidth, fontheight = textsize('100, 100, 100, 100 :', font=font)\n",
    "    # Prepare the composite image\n",
    "    max_width, total_height = calculate_composite_dimensions(boxes, horizontal_padding, vertical_buffer, target_text_height, fontwidth)\n",
    "    composite_img = Image.new('RGB', (max_width, total_height), median_color)\n",
    "    draw = ImageDraw.Draw(composite_img)\n",
    "    current_y = vertical_buffer\n",
    "    for i, (box, crop) in enumerate(zip(boxes, crops)):\n",
    "        enhancer = ImageEnhance.Contrast(crop)\n",
    "        enhanced_img = enhancer.enhance(2)\n",
    "        padded_img = ImageOps.expand(enhanced_img, border=(horizontal_padding, vertical_buffer), fill=median_color)\n",
    "        crop_width, crop_height = padded_img.size\n",
    "        # Text preparation\n",
    "        text = f\"{box[0]}, {box[1]}, {box[2]}, {box[3]} :\"\n",
    "        text_size = textsize(text, font=font)\n",
    "        text_x = minimal_gap\n",
    "        crop_y_centroid = current_y + crop_height // 2\n",
    "        text_y = crop_y_centroid - text_size[1] // 2\n",
    "        draw.text((text_x, text_y), text, fill=(0, 0, 0), font=font)\n",
    "        crop_x = text_size[0] + minimal_gap\n",
    "        composite_img.paste(padded_img.convert('L'), (crop_x, current_y))\n",
    "        current_y += crop_height + gap\n",
    "    temp_path = 'temp_composite.jpg'\n",
    "    composite_img.save(temp_path)\n",
    "    # OCR to extract text including coordinates\n",
    "    df_text = extract_easyocr(temp_path, mode=['LINE'])\n",
    "    extracted_text = df_text['text'].tolist()\n",
    "    parsed_text = parse_extracted_text(extracted_text)\n",
    "    return pd.DataFrame(parsed_text)\n",
    "\n",
    "def match_and_draw(image_path, df_textract, df_yolo_orig, mode='draw'):\n",
    "    df_textract = filter_boundingbox_output(df_textract)\n",
    "    relevant_classes = ['box', 'kv', 'signature', 'header']\n",
    "    df_yolo = filter_boundingbox_output(df_yolo_orig[df_yolo_orig['label'].isin(relevant_classes)])\n",
    "    df_yolo_table = filter_boundingbox_output(df_yolo_orig[df_yolo_orig['label'].isin(['table', 'column'])])\n",
    "    combined_data = []\n",
    "    matched_yolo_indices = set()  # Use a set for efficient look-up\n",
    "    unmatched_textract_indices = set(df_textract.index)  # Initialize with all textract indices\n",
    "    unmatched_yolo_boxes = []\n",
    "    raw_image = cv2.imread(image_path)\n",
    "    \n",
    "    if mode == 'draw':\n",
    "        original_image = raw_image.copy()\n",
    "        textract_image = raw_image.copy()\n",
    "        yolo_image = raw_image.copy()\n",
    "        \n",
    "        for _, row in df_textract.iterrows():\n",
    "            cv2.rectangle(textract_image, (row['x1'], row['y1']), (row['x2'], row['y2']), (0, 0, 255), 2)\n",
    "        \n",
    "        for _, row in df_yolo.iterrows():\n",
    "            if row['label'] in relevant_classes:\n",
    "                cv2.rectangle(yolo_image, (row['x1'], row['y1']), (row['x2'], row['y2']), (255, 0, 0), 2)\n",
    "        \n",
    "        for _, row in df_yolo_table.iterrows():\n",
    "            cv2.rectangle(original_image, (row['x1'], row['y1']), (row['x2'], row['y2']), (125, 0, 125), 3)\n",
    "    \n",
    "    for index_y, row_y in df_yolo.iterrows():\n",
    "        max_iou = 0\n",
    "        best_match_index_t = None\n",
    "        for index_t, row_t in df_textract.iterrows():\n",
    "            iou = get_iou([row_t['x1'], row_t['y1'], row_t['x2'], row_t['y2']], \n",
    "                          [row_y['x1'], row_y['y1'], row_y['x2'], row_y['y2']])\n",
    "            if iou > max_iou:\n",
    "                max_iou = iou\n",
    "                best_match_index_t = index_t\n",
    "\n",
    "        if max_iou > 0:\n",
    "            matched_yolo_indices.add(index_y)\n",
    "            if best_match_index_t is not None:\n",
    "                unmatched_textract_indices.discard(best_match_index_t)\n",
    "                combined_data.append({\n",
    "                    \"yolo_coords\": [row_y['x1'], row_y['y1'], row_y['x2'], row_y['y2']],\n",
    "                    \"textract_coords\": [[df_textract.loc[best_match_index_t][col] for col in ['x1', 'y1', 'x2', 'y2']]],\n",
    "                    \"textract_texts\": [df_textract.loc[best_match_index_t]['text']]\n",
    "                })\n",
    "            # Draw green boxes for matched YOLO boxes\n",
    "            if mode == 'draw':\n",
    "                cv2.rectangle(original_image, (row_y['x1'], row_y['y1']), (row_y['x2'], row_y['y2']), (0, 255, 0), 2)\n",
    "    # Draw red boxes for unmatched Textract boxes\n",
    "    for index_t in unmatched_textract_indices:\n",
    "        row_t = df_textract.loc[index_t]\n",
    "        combined_data.append({\n",
    "            \"yolo_coords\": [],\n",
    "            \"textract_coords\": [[row_t['x1'], row_t['y1'], row_t['x2'], row_t['y2']]],\n",
    "            \"textract_texts\": [row_t['text']]\n",
    "        })\n",
    "        if mode == 'draw':\n",
    "            cv2.rectangle(original_image, (row_t['x1'], row_t['y1']), (row_t['x2'], row_t['y2']), (0, 0, 255), 2)\n",
    "\n",
    "    # Draw Blue boxes for unmatched YOLO boxes of relevant classes\n",
    "    for index_y, row_y in df_yolo.iterrows():\n",
    "        if index_y not in matched_yolo_indices and row_y['label'] in relevant_classes:\n",
    "            unmatched_yolo_boxes.append([row_y['x1'], row_y['y1'], row_y['x2'], row_y['y2']])\n",
    "            if mode == 'draw':\n",
    "                cv2.rectangle(original_image, (row_y['x1'], row_y['y1']), (row_y['x2'], row_y['y2']), (255, 0, 0), 2)\n",
    "    \n",
    "    missed_boxes = []\n",
    "    for box in unmatched_yolo_boxes:\n",
    "        # Modify extract_text_from_image to suit this specific use case\n",
    "        missed_boxes.append(box)\n",
    "    if missed_boxes:\n",
    "        # Call the function to extract text for the missed boxes\n",
    "        df_missed_text = crop_and_extract_text(image_path, missed_boxes)\n",
    "        # Re-match the newly extracted texts with the YOLO boxes based on IOU\n",
    "        for index_y, row_y in df_yolo.iterrows():\n",
    "            if row_y['label'] in relevant_classes and index_y not in matched_yolo_indices:\n",
    "                max_iou = 0\n",
    "                best_match_index_t = None\n",
    "                for index_t, row_t in df_missed_text.iterrows():\n",
    "                    iou = get_iou([row_t['x1'], row_t['y1'], row_t['x2'], row_t['y2']], \n",
    "                                [row_y['x1'], row_y['y1'], row_y['x2'], row_y['y2']])\n",
    "                    #print([row_t['x1'], row_t['y1'], row_t['x2'], row_t['y2']], [row_y['x1'], row_y['y1'], row_y['x2'], row_y['y2']], iou)\n",
    "                    if iou > max_iou:\n",
    "                        max_iou = iou\n",
    "                        best_match_index_t = index_t\n",
    "\n",
    "                if max_iou > 0:\n",
    "                    matched_yolo_indices.add(index_y)\n",
    "                    combined_data.append({\n",
    "                        \"yolo_coords\": [row_y['x1'], row_y['y1'], row_y['x2'], row_y['y2']],\n",
    "                        \"textract_coords\": [[df_missed_text.loc[best_match_index_t][col] for col in ['x1', 'y1', 'x2', 'y2']]],\n",
    "                        \"textract_texts\": [df_missed_text.loc[best_match_index_t]['text']]\n",
    "                    })\n",
    "\n",
    "    if mode == 'draw':\n",
    "        for data in combined_data:\n",
    "            # Each entry in combined_data should have 'textract_coords' and 'textract_texts'\n",
    "            for coords, text in zip(data['textract_coords'], data['textract_texts']):\n",
    "                # Ensuring coords is a list with 4 elements [x1, y1, x2, y2]\n",
    "                if isinstance(coords, list) and len(coords) == 4:\n",
    "                    x1, y1, x2, y2 = coords\n",
    "                    position = (max(x1 - 10, 0), max((y1+y2)//2, 0))\n",
    "                    cv2.putText(original_image, text, position, cv2.FONT_HERSHEY_SIMPLEX, .7, (0, 125, 125), 1)\n",
    "        h_min = min(textract_image.shape[0], yolo_image.shape[0], original_image.shape[0])\n",
    "        textract_image = cv2.resize(textract_image, (int(textract_image.shape[1] * h_min / textract_image.shape[0]), h_min))\n",
    "        yolo_image = cv2.resize(yolo_image, (int(yolo_image.shape[1] * h_min / yolo_image.shape[0]), h_min))\n",
    "        original_image = cv2.resize(original_image, (int(original_image.shape[1] * h_min / original_image.shape[0]), h_min))\n",
    "        final_image = cv2.hconcat([textract_image, yolo_image, original_image])\n",
    "\n",
    "        # Save the combined image\n",
    "        output_image_path = 'results/' + os.path.basename(image_path).replace('.jpg', 'combined.jpg')\n",
    "        cv2.imwrite(output_image_path, final_image)\n",
    "    return pd.DataFrame(combined_data)\n",
    "\n",
    "\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "for image_path in tqdm(glob.glob('rectified/*.jpg')):\n",
    "    df_textract = extract_easyocr(image_path)\n",
    "    df_yolo = get_yolo_boxes(image_path)\n",
    "    match_and_draw(image_path, df_textract, df_yolo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f24de3-4490-4e54-ba28-b255298556d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
